# ðŸ§  MULTI AGENT RESEARCH SUMMARIZER(AI Paper Summarizer)

An automated pipeline to fetch, summarize, and explore the latest Artificial Intelligence research papers from **arXiv.org**, powered by **LangChain**, **FAISS**, and **Google Gemini embeddings**. Designed with a daily scheduler that updates the system at **11:50 PM IST**.

---

## ðŸš€ Features

- â° **Daily Paper Fetching:** Automatically fetches the 3 latest AI research papers from arXiv every night at **11:50 PM**.
- ðŸ“„ **PDF Download:** Downloads the full paper PDFs locally.
- âœ‚ï¸ **Text Summarization:** Uses an LLM (e.g., Gemini) to generate summaries from paper abstracts or full content.
- ðŸ” **Semantic Search:** Stores summaries in a FAISS vector database for similarity search and question answering.
- ðŸŒ **Streamlit UI:** Lets users view paper previews and ask questions about recent research papers.

---

## ðŸ§± Project Structure

â”œâ”€â”€ agents/
â”‚ â””â”€â”€ vector_store.py # Handles FAISS storage and Google Gemini embedding
â”œâ”€â”€ components/
â”‚ â”œâ”€â”€ scraper.py # Fetches latest AI papers from arXiv
â”‚ â”œâ”€â”€ downloader.py # Downloads PDFs of the papers
â”‚ â””â”€â”€ summarizer.py # Summarizes paper abstracts/full texts
â”œâ”€â”€ graph/
â”‚ â””â”€â”€ pipeline.py # LangGraph pipeline that runs the full process
â”œâ”€â”€ streamlit_app.py # UI for querying and viewing papers
â”œâ”€â”€ main.py # Entry script to run the full pipeline manually
â”œâ”€â”€ scheduler.py # Cron scheduler to auto-run main.py daily
â”œâ”€â”€ .env # API keys and sensitive config (use dotenv)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md # You are here


---

## ðŸ” Daily Workflow (Scheduled at 11:50 PM IST)

### ðŸ› ï¸ Trigger
A scheduler (`scheduler.py`) is configured using `schedule` or `cron` to invoke the pipeline daily at **11:50 PM**.

### ðŸ“¦ Pipeline Stages

#### 1. `scraper.py`
- Uses the `arxiv` Python library.
- Queries the **3 latest Artificial Intelligence** papers.
- Stores metadata like:
  - `title`
  - `abstract`
  - `published date`
  - `pdf_url`

#### 2. `downloader.py`
- Takes metadata from the scraper.
- Downloads each paperâ€™s PDF from the `pdf_url`.
- Stores them in `data/papers/`.

#### 3. `summarizer.py`
- Processes abstract (or content from the PDF if needed).
- Uses Gemini Pro or similar LLM to create a summary.
- Saves title, summary, abstract, and source URL.

#### 4. `vector_store.py`
- Embeds summaries using `GoogleGenerativeAIEmbeddings` from `langchain_google_genai`.
- Uses `FAISS` to store the vectorized documents.
- Saves the vector index to `faiss_index/`.

#### 5. `pipeline.py`
- Combines the above steps into a LangGraph node graph.
- Easily extendable to support multiple document types or sources.

---

## ðŸ’¡ Streamlit App: `streamlit_app.py`

This is the user-facing interface where:
- The 3 latest papers are listed with:
  - **Title**
  - **Abstract snippet**
  - **Source URL (PDF)**
- Users can enter queries like:
  - *â€œWhat is reinforcement learning?â€*
  - *â€œExplain the contribution of paper Xâ€*
- The app returns **semantic results** using vector similarity search.

---

## ðŸ” API Key Setup

You must add your **Gemini API key** to `.env` file:


Or, on **Streamlit Cloud**, add it in `st.secrets`.

---

## âœ… How to Run Manually
bash
Step 1: Install dependencies
pip install -r requirements.txt

 Step 2: Set up .env
echo "GOOGLE_API_KEY=your_key_here" > .env

 Step 3: Run full pipeline
python main.py

 Step 4: Run Streamlit UI
streamlit run streamlit_app.py
python scheduler.py
